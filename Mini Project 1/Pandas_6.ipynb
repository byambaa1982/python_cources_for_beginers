{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/byambaa1982/python_cources_for_beginers/blob/class_zero_burentugs/Mini%20Project%201/Pandas_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Working On CSV file\n"
      ],
      "metadata": {
        "id": "NNpzcNqt0GLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "r2CCFUILV_iN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Turn CSV file into Dataframe"
      ],
      "metadata": {
        "id": "TWfZH3CEG0Kb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"twitter_data.csv\")\n",
        "print(df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "RCX3D3NiElLA",
        "outputId": "7741452b-c3f1-426b-d783-a5cc2f42ebcc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'twitter_data.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-f1d39775c55f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"twitter_data.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1706\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    864\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'twitter_data.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "tXsrIpNMHwFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(2024-01-19-10-29)"
      ],
      "metadata": {
        "id": "qq6gNRrTQ8lO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['text']"
      ],
      "metadata": {
        "id": "j-RODKYuQxPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(df['created_at'][0])"
      ],
      "metadata": {
        "id": "g_GZO0NwMTFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1\n",
        "\n",
        "- Create new dataframe. Columns: username, keyword, liked, modified"
      ],
      "metadata": {
        "id": "vNSWqmfSJMND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = df[['username', 'keyword', 'liked', 'modified']]"
      ],
      "metadata": {
        "id": "9kzs6ZmbImHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.shape"
      ],
      "metadata": {
        "id": "5UEoLvMgMOm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Homework\n",
        "\n",
        "1. Show how many different keywords?\n",
        "2. Show the most frequent keyword.\n",
        "3. Create df3 columns: author_id, text, and df4 author_id, username and then join them on author_id name joined_df\n"
      ],
      "metadata": {
        "id": "DIA3-TUxKRT0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['keyword'].unique()"
      ],
      "metadata": {
        "id": "O7zVQDhcJuGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['keyword'].value_counts()"
      ],
      "metadata": {
        "id": "G3ThQqDJK_IF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = df[['author_id', 'username']]\n",
        "only_head = df2.head()\n",
        "only_head"
      ],
      "metadata": {
        "id": "HW7VCTEVJrCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "only_head.to_csv('test_file.csv', index = False)"
      ],
      "metadata": {
        "id": "gYJh7ITaVAvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new = df.loc[[1, 3, 4,5, 6, 7, 8], ['author_id', 'username', 'text']]\n",
        "new"
      ],
      "metadata": {
        "id": "tEm1ne6-KyJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new.to_csv('new_file.csv', index = False)"
      ],
      "metadata": {
        "id": "hV-fB0mwVfhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df = df[df['username']=='matthewgiglia']\n",
        "new_df"
      ],
      "metadata": {
        "id": "K6YNc7dLTsij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df[['created_at', 'author_id', 'keyword']]"
      ],
      "metadata": {
        "id": "S6ER3_p1UV7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[df['username']=='matthewgiglia', ['created_at', 'author_id', 'keyword']]"
      ],
      "metadata": {
        "id": "jwXwLm0DQNdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "56vaeVCBTpGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[2]"
      ],
      "metadata": {
        "id": "QtVQTVs6UJ5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Homework\n",
        "\n",
        "1. create 3 different tables each has one common column and 2 different columns\n",
        "2. join them back\n",
        "3. filter only keyword 'databricks'\n",
        "4. filter only keyword 'pyspark' and show how many rows."
      ],
      "metadata": {
        "id": "qLv7iRtLVp-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_to_df = pd.read_csv('new_file.csv')\n",
        "csv_to_df"
      ],
      "metadata": {
        "id": "NCXBlwyGUicL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l3zYxKf2A3De"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.DataFrame({\n",
        "    'author_id': [182918785, 986230825107812352, 1251929728904290304, 1394746741, 1876561765, 967730353522839552, 1804119532342247424],\n",
        "    'username': ['TBRinc', 'SalesTechStar', 'TechCryptoGuy', 'bpcs_tech', 'BricksterGary', 'elias34976279', 'DavidsonMa41390'],\n",
        "    'text': [\n",
        "        'Databricks is bringing an added layer of value to the BI stack https://t.co/xBSMKYXd0f #Databricks #AI #BI #GenAI #dataintelligence https://t.co/dAl1zyhxKa',\n",
        "        'End-user survey finds Snowflake, Databricks, and cloud giants vying for cloud data warehousing supremacy https://t.co/60d0kcJ8mP #salestraining #technology #entrepreneurship #sales #marketing #business #branding',\n",
        "        'Ontology is the biggest advantage of $PLTR after I finally understand what it does. I am a heavy Databricks user, yes it can also enable the usage of GenAI, but all I can do with it is some fancy demos to impress managers, the real value is quite low. Implementing those kinds ofâ€¦ https://t.co/9xBeNiJQJb https://t.co/COFiZVB4SO',\n",
        "        'Blueprint Builder, Jim Donahoe, will be speaking at #DataSaturday Aug 17th. Don\\'t miss his session on the fundamentals of @Databricks. Whether you\\'re looking to deepen your knowledge or stay ahead in the data space, this event is a must-attend. ðŸ”¥ https://t.co/loy4hVAbfg https://t.co/HPuPvF8tei',\n",
        "        'The blog explores data streams from NASA satellites using @apachekafka and @databricks. It demonstrates ingestion and transformation with Delta Live Tables in SQL and AI/BI-powered analysis of supernova events. Check it out https://t.co/lOd2xD43a8',\n",
        "        '@kargarage Microsoft SQL Server vs Microsoft Azure Databricks: Choosing #bitcointechnology Right Data Platform for Your Business. In the realm of data management and analytics, organizations are often #decentralizedfinance with the decision of selecting the most suitable platform to meetâ€¦ https://t.co/Beln3xEBDN',\n",
        "        'How Oracle Autonomous Database Connects with Databricks Across Clouds. In the old days, customers had #bitcointechnology choose one processing system and put all their data #bitcoinexchange This could be a database or a Hadoop distribution. Trying to make these platforms wor'\n",
        "    ]\n",
        "})\n",
        "df2 = pd.DataFrame({\n",
        "    'author_id': [182918785, 986230825107812352, 1251929728904290304, 1394746741, 1876561765, 967730353522839552, 1804119532342247424],\n",
        "    'follower_count': ['5300', '120', '74000', '30', '265000', '7', '920'],\n",
        "    'ethnicity': ['German', 'American', 'Chinese', 'American', 'Mongolian', 'Russian', 'British']\n",
        "})\n",
        "df3 = pd.DataFrame({\n",
        "    'author_id': [182918785, 986230825107812352, 1251929728904290304, 1394746741, 1876561765, 967730353522839552, 1804119532342247424],\n",
        "    'age': ['24', '15', '13', '28', '37', '16', '22'],\n",
        "    'most_liked_post': ['Tutorial', 'Animal_video', 'Reaction_video', 'Gameplay', 'POV', 'Roleplay', 'POV']\n",
        "})"
      ],
      "metadata": {
        "id": "KBgRTP50_1HS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = pd.merge(df1, df2, on='author_id')"
      ],
      "metadata": {
        "id": "uau5OJIA_1CT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = pd.merge(merged_df, df3, on='author_id')"
      ],
      "metadata": {
        "id": "uzBXvqQg_06P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Merged DataFrame:\")\n",
        "print(merged_df)"
      ],
      "metadata": {
        "id": "TcsORODsD0pp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_df = df[df['text'].str.contains('Databricks')]\n",
        "print(filtered_df)"
      ],
      "metadata": {
        "id": "1otBBLeO9n9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pyspark_df = merged_df[merged_df['text'].str.contains('PySpark')"
      ],
      "metadata": {
        "id": "rP5TgO8tD8bE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pyspark_count = len(pyspark_df)"
      ],
      "metadata": {
        "id": "f88adB0_EBt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nNumber of rows containing 'PySpark': {pyspark_count}\")"
      ],
      "metadata": {
        "id": "Kp-MAtpZEIkg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}